{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements Of Data Processing (2020S2) - Week 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping ##\n",
    "The BeautifulSoup library can be used to scrape data from a web page for processing and analysis.  You can find out more about BeautifulSoup at https://www.crummy.com/software/BeautifulSoup/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example extracts tennis scores from the 2019 ATP Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This example extracts tennis scores from the 2019 ATP Tour\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#Specify the page to download\n",
    "u = 'https://en.wikipedia.org/wiki/2019_ATP_Tour'\n",
    "page = requests.get(u)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "#Locate the section we're interested in.  Here we are after the second table after the 'ATP_ranking id'\n",
    "section = soup.find(id='ATP_ranking')\n",
    "results = section.findNext('table').findNext('table')\n",
    "\n",
    "#Iterate through all rows in the resultant table\n",
    "rows = results.find_all('tr')\n",
    "\n",
    "i = 0\n",
    "records = []\n",
    "\n",
    "#for row in rows[1:2]:\n",
    "#    cells = row.find_all('th')\n",
    "#    print(\"{0}, {1}, {2} ,{3}\".format(cells[0].text.strip(), cells[1].text.strip(), cells[2].text.strip(), cells[3].text.strip()))\n",
    "#    # column headers are #, Player, Points, Tours\n",
    "    \n",
    "for row in rows[2:]:\n",
    "    cells = row.find_all('td')\n",
    "    record = []\n",
    "    #print(\"{0}::{1}::{2}::{3}\".format(cells[0].text.strip(), cells[1].text.strip(), cells[2].text.strip(), cells[3].text.strip()))\n",
    "    # column value: 1::Rafael Nadal (ESP)::9,585::12\n",
    "    \n",
    "    #Removes junk characters from string and stores the result\n",
    "    ranking = int(unicodedata.normalize(\"NFKD\", cells[0].text.strip()))\n",
    "    record.append(int(ranking))\n",
    "    \n",
    "    player = unicodedata.normalize(\"NFKD\", cells[1].text.strip())\n",
    "    #Removes the country from the player name, removing surrounding whitespaces.\n",
    "    player_name = re.sub('\\(.*\\)', '', player).strip()\n",
    "    #print(player_name)\n",
    "    record.append(player_name)\n",
    "\n",
    "    #Remove the thousands separator from the points value and store as an integer\n",
    "    points = unicodedata.normalize(\"NFKD\", cells[2].text.strip())\n",
    "    record.append(int(re.sub(',', '', points)))\n",
    "    \n",
    "    # number of tours: integer type\n",
    "    tours = unicodedata.normalize(\"NFKD\", cells[3].text.strip())\n",
    "    record.append(int(tours))\n",
    "    \n",
    "    #Store the country code separately\n",
    "    country_code = re.search('\\((.*)\\)', player).group(1)\n",
    "    record.append(country_code)\n",
    "    #print(record)\n",
    "    #[1, 'Rafael Nadal', 9585, 12, 'ESP']\n",
    "    records.append(record)\n",
    "    i = i+1\n",
    "\n",
    "column_names = [\"ranking\", \"player\", \"points\", \"tours\", \"country\"]\n",
    "tennis_data = pd.DataFrame(records, columns = column_names)\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(tennis_data['player'], tennis_data['points'])\n",
    "plt.ylabel('points')\n",
    "plt.title(\"ATP Tour - player points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note on *unicodedata.normalize()*\n",
    "\n",
    "Web pages commonlu uses uncode encoding.\n",
    "\n",
    "Most ASCII characters are printable characters of the english alphabet such as abc, ABC, 123, ?&!, etc., represented as a number between 32 and 127.\n",
    "    \n",
    "Unicode represents most written languages and still has room for even more; this\n",
    "includes typical left-to-right scripts like English and even right-to-left scripts like Arabic. Chinese, Japanese, and the many other variants are also represented within Unicode\n",
    "ASCII has its equivalent within Unicode.\n",
    "\n",
    "\n",
    "\n",
    "In Unicode, several characters can be expressed in various way. \n",
    "For example, the character U+00C7 (LATIN CAPITAL LETTER C WITH CEDILLA) \n",
    "can also be expressed as the sequence U+0043 (LATIN CAPITAL LETTER C) U+0327 (COMBINING CEDILLA).\n",
    "\n",
    "The Unicode standard defines various normalization forms of a Unicode string, \n",
    "based on the definition of canonical equivalence and compatibility equivalence. \n",
    "\n",
    "The function ***unicodedata.normalize(\"NFKD\", unistr)*** \n",
    "will apply the compatibility decomposition, i.e. replace all compatibility characters with their equivalents.\n",
    "\n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unistr = u'\\u2460'\n",
    "print(\"{0} is the equivalent character of {1}\".format(unicodedata.normalize('NFKD', unistr), unistr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 1 </span>\n",
    "\n",
    "Produce a graph similar to the example above for the **2019 ATP Doubles Scores**.\n",
    "\n",
    "*First locate the section we're interested in.*\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution of exercise 1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Specify the page to download\n",
    "u = 'https://en.wikipedia.org/wiki/2019_ATP_Tour'\n",
    "page = requests.get(u)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "## complete the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawling ##\n",
    "\n",
    "This example implements a simplified Web crawler that traverses the site \n",
    "**[http://books.toscrape.com/](http://books.toscrape.com/)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "page_limit = 20\n",
    "\n",
    "#Specify the initial page to crawl\n",
    "base_url = 'http://books.toscrape.com/'\n",
    "seed_item = 'index.html'\n",
    "\n",
    "seed_url = base_url + seed_item\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "visited = {}; \n",
    "visited[seed_url] = True\n",
    "pages_visited = 1\n",
    "print(seed_url)\n",
    "\n",
    "#Remove index.html\n",
    "links = soup.findAll('a')\n",
    "seed_link = soup.findAll('a', href=re.compile(\"^index.html\"))\n",
    "#to_visit_relative = list(set(links) - set(seed_link))\n",
    "to_visit_relative = [l for l in links if l not in seed_link]\n",
    "\n",
    "\n",
    "# Resolve to absolute urls\n",
    "to_visit = []\n",
    "for link in to_visit_relative:\n",
    "    to_visit.append(urljoin(seed_url, link['href']))\n",
    "\n",
    "    \n",
    "#Find all outbound links on succsesor pages and explore each one \n",
    "while (to_visit):\n",
    "    # Impose a limit to avoid breaking the site \n",
    "    if pages_visited == page_limit :\n",
    "        break\n",
    "        \n",
    "    # consume the list of urls\n",
    "    link = to_visit.pop(0)\n",
    "    print(link)\n",
    "\n",
    "    # need to concat with base_url, an example item <a href=\"catalogue/sharp-objects_997/index.html\">\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # scarping code goes here\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # mark the item as visited, i.e., add to visited list, remove from to_visit\n",
    "    visited[link] = True\n",
    "    to_visit\n",
    "    new_links = soup.findAll('a')\n",
    "    for new_link in new_links :\n",
    "        new_item = new_link['href']\n",
    "        new_url = urljoin(link, new_item)\n",
    "        if new_url not in visited and new_url not in to_visit:\n",
    "            to_visit.append(new_url)\n",
    "        \n",
    "    pages_visited = pages_visited + 1\n",
    "\n",
    "print('\\nvisited {0:5d} pages; {1:5d} pages in to_visit'.format(len(visited), len(to_visit)))\n",
    "#print('{0:1d}'.format(pages_visited))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 2 </span>\n",
    "\n",
    "The code above can easily be end up stuck in a crawler trap.  \n",
    "Explain three ways this could occur and suggest possible solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 3 </span>\n",
    "\n",
    "Modify the code above to print the titles of as many books as can be found within the page_limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise 3\n",
    "# Modify the code above to print the titles of as many books as can be found within the page_limit\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# complete the program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
